{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c322e2ea",
   "metadata": {},
   "source": [
    "# Project2: Anomaly Detection for Exotic Event Identification at the Large Hadron Collider \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225d014c",
   "metadata": {},
   "source": [
    "## Brief Introduction to the Standard Model and Large Hadron Collider\n",
    "\n",
    "\n",
    "The Standard model (SM) of Particle Physics is the most complete model physicists have for understanding the interactions of the fundamental particles in the universe. The elementary particles of the SM are shown in Fig.1.\n",
    "\n",
    "---\n",
    "<figure>\n",
    "    <img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/0/00/Standard_Model_of_Elementary_Particles.svg/627px-Standard_Model_of_Elementary_Particles.svg.png\" alt=\"SM\" style=\"width: 600px;\"/>\n",
    "    <figcaption>Fig.1 - Elementary particles of the Standard Model.</figcaption>\n",
    "</figure>\n",
    "\n",
    "---\n",
    "\n",
    "It is comprised of matter particles (**fermions**):\n",
    "- **leptons**\n",
    "    - electrons\n",
    "    - muon\n",
    "    - tau\n",
    "    - and respective neutrinos\n",
    "- **quarks** which are the building blocks of protons\n",
    "\n",
    "as well as force carrier particles (**bosons**):\n",
    "- photon and W/Z bosons (electroweak force)\n",
    "- gluons (strong force)\n",
    "\n",
    "and the Higgs boson which is attributed to the mechanism which gives particles their mass.\n",
    "\n",
    "\n",
    "Though the SM has experimentally stood the test of time, many outstanding questions about the universe and the model itself remain, and scientist continue to probe for inconsistencies in the SM in order to find new physics. More exotic models such as **Supersymmetry (SUSY)** predic mirror particles which may exist and have alluded detection thus far. \n",
    "\n",
    "---\n",
    "\n",
    "The **Large Hadron Collider** (LHC) is a particle smasher capable of colliding protons at a centre of mass energy of 14 TeV.\n",
    "**ATLAS** is general purpouse particle detectors tasked with recording the remnants of proton collisions at the collicion point. The main purpouse of this experiment is to test the SM rigorously, and ATLAS was one of two expeririments (ATLAS+CMS) responsible for the discovery of the **Higgs boson in 2012**. \n",
    "\n",
    "Find an animation of how particles are reconstructed within a slice of the ATLAS detector here: https://videos.cern.ch/record/2770812. Electrons, muons, photons, quark jets, etc, will interact with different layers of the detector in different ways, making it possible to design algorithms which distinguish reconstructed particles, measure their trajectories, charge and energy, and identify them as particular types.\n",
    "\n",
    "Figure 2 shows an event display from a data event in ATLAS in which 2 muons (red), 2 electrons (green), and 1 quark-jet (purple cone) are found. This event is a candidate to a Higgs boson decaying to four leptons with an associated jet: $$H (+j)\\rightarrow 2\\mu 2e (+j)$$ \n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "<figure>\n",
    "    <img src=\"https://twiki.cern.ch/twiki/pub/AtlasPublic/EventDisplayRun2Physics/JiveXML_327636_1535020856-RZ-LegoPlot-EventInfo-2017-10-18-19-01-24.png\" alt=\"Higgs to leptons\" style=\"width: 600px;\"/>\n",
    "    <figcaption>Fig.2 - Event display of a Higgs candidate decaying to two muons and two electrons.</figcaption>\n",
    "</figure>\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "Particles are shown transversing the detector material. The 3D histogram show \n",
    "* the azimuth $\\phi$ ( angle around the beam, 0 is up)\n",
    "* pseudo-rapidity $\\eta$ (trajectory along the beam) positions of the particle directions with respect to the interaction point.\n",
    "* The total energy measured for the particle is denoted by $E$,\n",
    "* the transverse momentum ($p_T$) deposited by the particle in giga-electronvolts (GeV) are shown by the hight of the histograms.\n",
    "\n",
    "A particle kinematics can then be described by a four-vector  $$\\bar{p} = (E,p_T,\\eta,\\phi)$$\n",
    "\n",
    "An additional importan quantity is the missing energy in the transverse plane (MET). This is calculated by taking the negative sum of the transverse momentum of all particles in the event.\n",
    "$$\\mathrm{MET} = -\\sum p_T$$\n",
    "\n",
    "With perfect detector performance the MET will sum to 0 if all outgoing particles are observed by the detector. Neutrinos cannot be measured by the detector and hence their precense produces non-zero MET."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655ada0c",
   "metadata": {},
   "source": [
    "## Anomally detection dataset\n",
    "\n",
    "For the anomally detection project we will use the dataset discussed in this publication: <p><a href=\"https://arxiv.org/pdf/2105.14027.pdf\" title=\"Anomalies\">The Dark Machines Anomaly Score Challenge:\n",
    "Benchmark Data and Model Independent Event\n",
    "Classification for the Large Hadron Collider</a></p>\n",
    "\n",
    "Familiarise yourself with the paper, in particular from sections 2.1 to 4.4.\n",
    "\n",
    "---\n",
    "\n",
    "The dataset contains a collection of simulated proton-proton collisions in a general particle physics detector (such as ATLAS). We will use a dataset containing `340 000` SM events (referred to as channel 2b in the paper) which have at least 2 electrons/muons in the event with $p_T>15$ GeV. \n",
    "\n",
    "**The events can be found in `background_chan2b_7.8.csv`**\n",
    "\n",
    "\n",
    "You can see all the SM processes that are simulated in Table 2 of the paper, \n",
    "\n",
    "    e.g., an event with a process ID of `w_jets` is a simulated event of two protons producing a lepton and neutrino and at least two jets.\n",
    "    \n",
    "$$pp\\rightarrow \\ell\\nu(+2j)$$\n",
    "\n",
    "---\n",
    "\n",
    "The datasets are collected as CSV files where each line represents a single event, with the current format:\n",
    "\n",
    "`event ID; process ID; event weight; MET; METphi; obj1, E1, pt1, eta1, phi1; obj2, E2, pt2, eta2, phi2; ...`\n",
    "See Section 2.2 for a description of the dataset.\n",
    "Variables are split by a semicolon `\";\"`\n",
    "- `event ID`: an identifier for the event number in the simulation\n",
    "- `process ID`: an identifier for the event simulation type\n",
    "- `event weight`: the weight associated to the simulated event (how important that event is)\n",
    "- `MET`: the missing transverse energy\n",
    "- `METphi`: the azimuth angle (direction) of the MET\n",
    "\n",
    "the a list of objects (particles) whose variables are split by commas `\",\"` in the following orger:\n",
    "- `obj`: the object type,\n",
    "\n",
    "    |Key|Particle|\n",
    "    |---|---|\n",
    "    |j|jet|\n",
    "    |b|b-jet|\n",
    "    |e-|electron|\n",
    "    |e+|positron|\n",
    "    |m-|muon|\n",
    "    |m+|muon+|\n",
    "    |g|photon|\n",
    "    \n",
    "    *see Table 1 of the paper*\n",
    "- `E`: the total measured particle energy in MeV, [0,inf]\n",
    "- `pt`: the transverse mementum in MeV, [0,inf]\n",
    "- `eta`: pseudo-rapidity, [-inf,inf]\n",
    "- `phi`: azimuth angle, radians [-3.14,3.14]\n",
    "\n",
    "---\n",
    "\n",
    "In addition to the SM events we are also provided simulated events from `Beyond Standard Model` (BSM) exotic physics models. They are summarised here:\n",
    "\n",
    "|Model | File Name | \n",
    "|---|---|\n",
    "|**SUSY chargino-chargino process**||\n",
    "||`chacha_cha300_neut140_chan2b.csv`|\n",
    "||`chacha_cha400_neut60_chan2b.csv`|\n",
    "||`chacha_cha600_neut200_chan2b.csv`|\n",
    "|**SUSY chargino-neutralino processes**||\n",
    "||`chaneut_cha200_neut50_chan2b.csv`|\n",
    "||`chaneut_cha250_neut150_chan2b.csv`|\n",
    "|**$Z'$ decay to leptons**||\n",
    "||`pp23mt_50_chan2b.csv`|\n",
    "||`pp24mt_50_chan2b.csv`|\n",
    "|**Gluino and RPV SUSY**||\n",
    "||`gluino_1000.0_neutralino_1.0_chan2b.csv`||\n",
    "||`stlp_st1000_chan2b.csv`||\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686809e6",
   "metadata": {},
   "source": [
    "## Project description\n",
    "\n",
    "### Overview\n",
    "The task is to design an anomaly detection algorithm which is trained on the SM dataset and which can be used to flag up interesting (exotic) events from the BSM physics models.\n",
    "\n",
    "You will do this by designing a robust `AutoEncoder` which is trained on the event level variables `MET; METphi` and the kinematics of the particle level objects. The `AutoEncoder` needs to duplicate the input as output effectively while going through a laten space (bottleneck). \n",
    "\n",
    "You will then need to evaluate and discuss the performance of your `AutoEncoder` on the exotic models listed above, and come up with an appropiate metric to identify events from non SM physics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63d3beb",
   "metadata": {},
   "source": [
    "# **Breakdown**\n",
    "\n",
    "In the project report you will be assessed in the following way.\n",
    "\n",
    "1. **Data exploration and preprocessing (20%):** Inspect the datasets; visualise the data (e.g. tables, plots, etc) in an appropriate way; study the composition of the dataset; perform any necessary preprocessing.\n",
    "2. **Model selection (30%):** Choose a promissing approach; construct the machine learning model; optimise the relevant hyperparameters; train your chosen model.\n",
    "3. **Performance evaluation (30%):** Evaluate the model in a way that gauges its ability to generalise to unseen data; compare to other approaches; identify the best approach. \n",
    "4. **Discussion, style throughout (20%):** Discuss the reasoning or intuition behind your choices; the results you obtain through your studies; the relative merits of the methods you have developed, _etc._ Similarly, make sure that you write efficient code, document your work, clearly convey your results, and convince us that you have mastered the material.\n",
    "\n",
    "\n",
    "## Data Preprocessing\n",
    "* The data is provided in a CSV (text) format with semicolon and comma seperated list with **one line per event**. We need to convert this into an appropiate format for our neural networks. \n",
    "* Since the number of particles per event is variable you will need to **truncate** and **mask** particles in the event. The following steps need to be perfomed on the SM (background) sample:\n",
    "     1. Create variables where you count the number of electrons, photons, muons, jets and bjets in the event (ignore charge) before any truncation.\n",
    "     2. Choose an appropiate number of particles to study per event (recommended: **8** particles are used in the paper)\n",
    "     3. Check the particles are sorted by energy (largest to smallest)\n",
    "     4. If the event has more than 8 particles choose the **8 particles** with **highest energy and truncate** the rest.\n",
    "     5. convert energy and momentum variables by logarithm (e.g., `log`) - this is to prioritise differences in energy **scale** over more minor differences. \n",
    "     6. If the event has less than 8 particles, create kinematic variables with 0 values for the missing particles.\n",
    "* The final set of training variables should look something like this (the exact format is up to you)\n",
    "    |N ele| N muon| N jets| N bjets| N photons| log(MET)| METphi| log(E1)| log(pt1)| eta1| phi1| ... | phi8|\n",
    "    |-|-|-|-|-|-|-|-|-|-|-|-|-|\n",
    "    \n",
    "    7. After the dataset is ready, use `MinMaxScalar` or similar to standardise the training variables over the SM dataset\n",
    "* After the SM dataset has been processed use the same processing on the BSM (signal samples). Use the same standardisation functions as on the SM dataset, *Do not recalculate the standardisation*.\n",
    "* Keep associated metatata (`event ID; process ID; event weight;`) though this does not need processing. \n",
    "* Randomise and split the SM (background) dataset into training and testing datasets (the BSM samples don't need to be split (*Why?*))\n",
    "* *Hint*: It is suggested that you write a class or function for the preprocessing which takes a csv path as input and provides the processed dataset. After you have done the data processing its suggested you save the datasets so as to not have to recalculate them again if the kernel is restarted. \n",
    "\n",
    "## Training\n",
    "* Design an appropiate algorithm which reconstrucuts the input variables after going though a laten space. Choose an appropiate cost function.\n",
    "    * The suggested method for ease of implementation is the `AutoEncoder`\n",
    "    * However, if you consider learning about or trying something else, as described in the paper, you should feel welcome to try `VAEs`, `ConvAEs`, `ConvVAEs`, etc. Don't feel you **have** to create an `AE`.\n",
    "\n",
    "* Explore different architectures for the model, and explain in detail your choice of model, and the final parameters chosen.\n",
    "* It is suggested to create a class or function around your algorithm which allows you to easily tweek hyperparameters of the model (depth, number of nodes, number of laten variables, activation functions, regularisation, etc)\n",
    "* Train the model over several parameters to find the best algorithm. Document the process throught and discuss your choices. Keep track of validation performance. Save the models the best points. \n",
    "* Explore the results and document your findings. Ask as many questions about your model as you can, and document your findings. Does the model generalise well to data it hasn't seen?\n",
    "\n",
    "## Evaluation\n",
    "In the evaluation explore different datasets an try answer as many questions about the performance as possible. \n",
    "* Evaluate the performance of the `AE` on BSM dataset. Which models are more or less similar to the SM?\n",
    "* Explore the anomaly score as a handle on finding new physics. Consider scanning over different anomaly scores and calculating the signal and background efficiencies at each point (plot this for different BSM models). How might you choose a value which flags up a non-SM event? \n",
    "* Explore SM events. Which look more anomolous than others? Are there any particular features which are responsible, e.g. particle counts, MET ranges, etc.? \n",
    "* Discuss any limitations your algorithm has. How might you update and improve your model in future? Discuss any issues you had, or things you would have liked to try given more time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b5d8e2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "To complete this project, you should **Submit your Jupyter notebook** as a \"report.\" See the comments below on documentation,\n",
    "\n",
    "\n",
    "\n",
    "**You should submit by Friday 10th Feb 2023 at 10AM:**\n",
    "* your report notebook via Turnitin.\n",
    "    \n",
    "\n",
    "For all task we're not looking for exceptional performace and high scores (although those are nice too), **we're mostly concerned with _best practices:_** If you are careful and deliberate in your work, and show us that you can use the tools introduced in the course so far, we're happy!\n",
    "\n",
    "Training all of these models in sequence takes a very long time so **don't spend hours on training hundreds of epochs.** Be conservative on epoch numbers (30 is more than enough) and use appropiate techniques like EarlyStopping to speed things up. Once you land on a good model you can allow for longer training times if performance can still improve.\n",
    "\n",
    "\n",
    "\n",
    "### Documentation\n",
    "\n",
    "**Change the filename to contain Name_Surname**\n",
    "\n",
    "Your report notebook should run without errors and give (mostly) reproducible results. **Please dont clear the report before submitting**! It is important that **all** code is annotated and that you provide brief commentary **at each step** to explain your approach. Explain *why* you chose a given approach and *discuss* the results. You can also include any failed approaches if you provide reasonable explanation; we care more about you making an effort and showing that you understand the core concepts.\n",
    "\n",
    "This is not in the form of a written report so do not provide pages of background material, but do try to clearly present your work so that the markers can easily follow your reasoning and can reproduce each of the steps through your analysis. Aim to convince us that you have understood the material covered in the course.\n",
    "\n",
    "To add commentary above (or below) a code snippet create a new cell and add your text in \"Markdown\" format. Do not add any substantial commentary as a code comment in the same cell as the code. To change the new cell into markdown select from the drop down menu on the bar above the main window (the default is code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34764f35",
   "metadata": {},
   "source": [
    "# Happy Anomaly Hunting\n",
    "---\n",
    "<blockquote class=\"twitter-tweet\"><p lang=\"en\" dir=\"ltr\">Data Scientist (n.): Person who is better at statistics than any software engineer and better at software engineering than any statistician.</p>&mdash; Josh Wills (@josh_wills) <a href=\"https://twitter.com/josh_wills/status/198093512149958656?ref_src=twsrc%5Etfw\">May 3, 2012</a></blockquote> <script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script> \n",
    "\n",
    "---\n",
    "\n",
    "Your code follows...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "947bc7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "475f15dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataRead():\n",
    "    def __init__(self, csv_path):\n",
    "        self.csv_path = csv_path\n",
    "        self.data = pd.read_csv(self.csv_path,ignore_errors=True)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fb9cb243",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\"Event ID\",\"Process ID\",\"Event Weight\",\n",
    "          \"N ele\",\"N muon\",\"N jets\",\"N bjets\",\"log(MET)\",\"METphi\"]\n",
    "for i in range(8):\n",
    "    i = str(i)\n",
    "    labels.extend([\"log(E\"+i+\")\",\"log(pt\"+i+\")\",\"eta\"+i,\"phi\"+i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "84d6b383",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Event ID</th>\n",
       "      <th>Process ID</th>\n",
       "      <th>Event Weight</th>\n",
       "      <th>N ele</th>\n",
       "      <th>N muon</th>\n",
       "      <th>N jets</th>\n",
       "      <th>N bjets</th>\n",
       "      <th>log(MET)</th>\n",
       "      <th>METphi</th>\n",
       "      <th>log(E0)</th>\n",
       "      <th>...</th>\n",
       "      <th>eta5</th>\n",
       "      <th>phi5</th>\n",
       "      <th>log(E6)</th>\n",
       "      <th>log(pt6)</th>\n",
       "      <th>eta6</th>\n",
       "      <th>phi6</th>\n",
       "      <th>log(E7)</th>\n",
       "      <th>log(pt7)</th>\n",
       "      <th>eta7</th>\n",
       "      <th>phi7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Event ID, Process ID, Event Weight, N ele, N muon, N jets, N bjets, log(MET), METphi, log(E0), log(pt0), eta0, phi0, log(E1), log(pt1), eta1, phi1, log(E2), log(pt2), eta2, phi2, log(E3), log(pt3), eta3, phi3, log(E4), log(pt4), eta4, phi4, log(E5), log(pt5), eta5, phi5, log(E6), log(pt6), eta6, phi6, log(E7), log(pt7), eta7, phi7]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 41 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(columns=labels)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3992c63d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "j,335587,132261,-1.57823,1.02902\n",
      "['5702564', 'z_jets', '1']\n"
     ]
    }
   ],
   "source": [
    "# open file and read lines in a for loop\n",
    "\n",
    "with open('background_chan2b_7.8.csv', 'r') as f:\n",
    "    for line in f:\n",
    "        #split each line with semi colon and comma\n",
    "     \n",
    "        line = line.split(';')\n",
    "    \n",
    "        metadata = [line[0], line[1], line[2]]\n",
    "        print(line[5])\n",
    "        analys_data = [line.count('e-'), line.count('m-'), line.count('j'), line.count('b'),np.log(float(line[3])), line[4]]\n",
    "        trun = line[8:]\n",
    "        print(metadata)\n",
    "        break\n",
    "        # df = df.append(pd.Series([,]), ignore_index=True)\n",
    "    \n",
    "        \n",
    "  \n",
    "\n",
    "           \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0e210098",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5702564</td>\n",
       "      <td>z_jets</td>\n",
       "      <td>1</td>\n",
       "      <td>102549.0</td>\n",
       "      <td>-2.966200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13085335</td>\n",
       "      <td>z_jets</td>\n",
       "      <td>1</td>\n",
       "      <td>103468.0</td>\n",
       "      <td>1.961930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>74025</td>\n",
       "      <td>wtopbar</td>\n",
       "      <td>1</td>\n",
       "      <td>129408.0</td>\n",
       "      <td>-1.178890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2419445</td>\n",
       "      <td>z_jets</td>\n",
       "      <td>1</td>\n",
       "      <td>77774.2</td>\n",
       "      <td>-1.091710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>43639</td>\n",
       "      <td>wtop</td>\n",
       "      <td>1</td>\n",
       "      <td>107151.0</td>\n",
       "      <td>-1.026420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340263</th>\n",
       "      <td>30</td>\n",
       "      <td>ttbar</td>\n",
       "      <td>1</td>\n",
       "      <td>65677.3</td>\n",
       "      <td>-1.153600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340264</th>\n",
       "      <td>111</td>\n",
       "      <td>ttbar</td>\n",
       "      <td>1</td>\n",
       "      <td>58730.1</td>\n",
       "      <td>0.529769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340265</th>\n",
       "      <td>75</td>\n",
       "      <td>ttbar</td>\n",
       "      <td>1</td>\n",
       "      <td>342729.0</td>\n",
       "      <td>0.804597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340266</th>\n",
       "      <td>15181306</td>\n",
       "      <td>z_jets</td>\n",
       "      <td>1</td>\n",
       "      <td>246999.0</td>\n",
       "      <td>-0.849401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340267</th>\n",
       "      <td>19552503</td>\n",
       "      <td>z_jets</td>\n",
       "      <td>1</td>\n",
       "      <td>109060.0</td>\n",
       "      <td>1.489960</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>340268 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               0        1  2         3         4\n",
       "0        5702564   z_jets  1  102549.0 -2.966200\n",
       "1       13085335   z_jets  1  103468.0  1.961930\n",
       "2          74025  wtopbar  1  129408.0 -1.178890\n",
       "3        2419445   z_jets  1   77774.2 -1.091710\n",
       "4          43639     wtop  1  107151.0 -1.026420\n",
       "...          ...      ... ..       ...       ...\n",
       "340263        30    ttbar  1   65677.3 -1.153600\n",
       "340264       111    ttbar  1   58730.1  0.529769\n",
       "340265        75    ttbar  1  342729.0  0.804597\n",
       "340266  15181306   z_jets  1  246999.0 -0.849401\n",
       "340267  19552503   z_jets  1  109060.0  1.489960\n",
       "\n",
       "[340268 rows x 5 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('background_chan2b_7.8.csv', sep=';', header=None, usecols=np.arange(0, 5))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a5d39871",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['chaneut_cha250_neut150_chan2b.csv',\n",
       " 'chacha_cha300_neut140_chan2b.csv',\n",
       " 'background_chan2b_7.8.csv',\n",
       " 'chacha_cha600_neut200_chan2b.csv',\n",
       " 'gluino_1000.0_neutralino_1.0_chan2b.csv',\n",
       " 'pp24mt_50_chan2b.csv',\n",
       " 'chaneut_cha200_neut50_chan2b.csv',\n",
       " 'pp23mt_50_chan2b.csv',\n",
       " 'stlp_st1000_chan2b.csv',\n",
       " 'chacha_cha400_neut60_chan2b.csv']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print all file names in the directory with the .csv extension\n",
    "import glob\n",
    "import os\n",
    "\n",
    "path = os.getcwd()\n",
    "file_name  = []\n",
    "for file in glob.glob(\"*.csv\"):\n",
    "    file_name.append(file)\n",
    "    \n",
    "\n",
    "file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8345a515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chaneut_cha250_neut150_chan2b.csv\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Usecols do not match columns, columns expected but not found: [11, 12, 13, 14]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [58], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m file_name:\n\u001b[1;32m      2\u001b[0m     \u001b[39mprint\u001b[39m(f)\n\u001b[0;32m----> 3\u001b[0m     df_p \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(f, sep\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m;\u001b[39m\u001b[39m'\u001b[39m, header\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, usecols\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39marange(\u001b[39m5\u001b[39m, \u001b[39m15\u001b[39m))\n\u001b[1;32m      4\u001b[0m     \u001b[39mlen\u001b[39m(df_p\u001b[39m.\u001b[39mcolumns)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/daml/lib/python3.9/site-packages/pandas/util/_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[39m=\u001b[39m new_arg_value\n\u001b[0;32m--> 211\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/daml/lib/python3.9/site-packages/pandas/util/_decorators.py:317\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[1;32m    312\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    313\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39marguments),\n\u001b[1;32m    314\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[1;32m    315\u001b[0m         stacklevel\u001b[39m=\u001b[39mfind_stack_level(inspect\u001b[39m.\u001b[39mcurrentframe()),\n\u001b[1;32m    316\u001b[0m     )\n\u001b[0;32m--> 317\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/daml/lib/python3.9/site-packages/pandas/io/parsers/readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    946\u001b[0m     defaults\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mdelimiter\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[1;32m    947\u001b[0m )\n\u001b[1;32m    948\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 950\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/daml/lib/python3.9/site-packages/pandas/io/parsers/readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    602\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[1;32m    604\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 605\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    607\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[1;32m    608\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/daml/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1439\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m   1441\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 1442\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/daml/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1747\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1744\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(msg)\n\u001b[1;32m   1746\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1747\u001b[0m     \u001b[39mreturn\u001b[39;00m mapping[engine](f, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions)\n\u001b[1;32m   1748\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[1;32m   1749\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/daml/lib/python3.9/site-packages/pandas/io/parsers/c_parser_wrapper.py:163\u001b[0m, in \u001b[0;36mCParserWrapper.__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m    160\u001b[0m     \u001b[39m# error: Cannot determine type of 'names'\u001b[39;00m\n\u001b[1;32m    161\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnames) \u001b[39m<\u001b[39m \u001b[39mlen\u001b[39m(usecols):  \u001b[39m# type: ignore[has-type]\u001b[39;00m\n\u001b[1;32m    162\u001b[0m         \u001b[39m# error: Cannot determine type of 'names'\u001b[39;00m\n\u001b[0;32m--> 163\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_usecols_names(\n\u001b[1;32m    164\u001b[0m             usecols,\n\u001b[1;32m    165\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnames,  \u001b[39m# type: ignore[has-type]\u001b[39;49;00m\n\u001b[1;32m    166\u001b[0m         )\n\u001b[1;32m    168\u001b[0m \u001b[39m# error: Cannot determine type of 'names'\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_parse_dates_presence(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnames)  \u001b[39m# type: ignore[has-type]\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/daml/lib/python3.9/site-packages/pandas/io/parsers/base_parser.py:918\u001b[0m, in \u001b[0;36mParserBase._validate_usecols_names\u001b[0;34m(self, usecols, names)\u001b[0m\n\u001b[1;32m    916\u001b[0m missing \u001b[39m=\u001b[39m [c \u001b[39mfor\u001b[39;00m c \u001b[39min\u001b[39;00m usecols \u001b[39mif\u001b[39;00m c \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m names]\n\u001b[1;32m    917\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(missing) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 918\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    919\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUsecols do not match columns, columns expected but not found: \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    920\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mmissing\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    921\u001b[0m     )\n\u001b[1;32m    923\u001b[0m \u001b[39mreturn\u001b[39;00m usecols\n",
      "\u001b[0;31mValueError\u001b[0m: Usecols do not match columns, columns expected but not found: [11, 12, 13, 14]"
     ]
    }
   ],
   "source": [
    "for f in file_name:\n",
    "    print(f)\n",
    "    df_p = pd.read_csv(f, sep=';', header=None, usecols=np.arange(5, 15))\n",
    "    len(df_p.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ad3ae0a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_p = pd.read_csv('background_chan2b_7.8.csv', sep=';', header=None, usecols=np.arange(5, 15))\n",
    "#numberr of columns in dataframe\n",
    "len(df_p.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf5cf36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "daml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 | packaged by conda-forge | (main, May 27 2022, 17:00:52) \n[Clang 13.0.1 ]"
  },
  "vscode": {
   "interpreter": {
    "hash": "19b09708295bbab47f2e7ec6e8227f8cc1a51947e806c9819c23d58f1093e901"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
